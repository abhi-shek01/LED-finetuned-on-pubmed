# LED-finetuned-on-pubmed

Hallucinations in text summarization occur when a model generates information that is not supported by the input source document. These hallucinations undermine the reliability and trustworthiness of the generated summaries. Reasons for hallucinations include biases in training data, lack of context understanding, or over-optimization of the model. Detecting and reducing hallucinations is crucial to ensure accurate and concise summaries.

One approach to reducing hallucinations is by generating named entities present in both the source document and abstract summary. By appending these entities to the summary, the model can focus more attention on them, resulting in a summary that includes relevant entities. Three new metrics, Precision-target, Precision-source, and Recall-target, are used to evaluate hallucination reduction.

The project focuses on reducing hallucinations in long scientific research documents using the PubMed dataset. Simple metrics are proposed to measure factual consistency at the entity level. The project analyzes the factual quality of summaries generated by the LED model (Longformer Encoder Decoder) and proposes techniques such as data filtering, multi-task learning, and joint sequence generation to improve performance on these metrics.

Language used: Python
Dataset used: Pubmed.
Samples Taken: 2500
Libraries used: Pandas, Numpy, Spacy(NER)
